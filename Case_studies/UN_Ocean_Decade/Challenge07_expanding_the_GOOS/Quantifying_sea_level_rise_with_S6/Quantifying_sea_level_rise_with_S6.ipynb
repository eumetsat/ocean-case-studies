{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General principle of computing one point of the global MSL ##\n",
    "\n",
    "The global Mean Sea Level is computed from the altimetric \"reference mission\", namely today (2023/04/19), Sentinel-6 Michael Freilich.\n",
    "We won't compute the whole time series here, but show the basis of computing one point in this along-time data series.\n",
    "\n",
    "<br>made for Eumetsat Copernicus Marine Data Service by V. Rosmorduc, CLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Import libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# to interpret paths and folder for any OS\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import shutil                                  # a library that allows us access to basic operating system commands\n",
    "# math library\n",
    "import numpy as np\n",
    "# downloading / reading / using NetCDF data\n",
    "import eumdac\n",
    "import xarray as xr\n",
    "# plotting library\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "#statistical library\n",
    "from scipy import stats\n",
    "#remove warnings for ease of reading\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#we will look at the plot within the notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a download directory for our products\n",
    "download_dir = os.path.join(os.getcwd(), \"products\")\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "download_data = False\n",
    "\n",
    "cycle = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "# load credentials\n",
    "    with open(os.path.join(os.path.expanduser(\"~\"),'.eumdac_credentials')) as json_file:\n",
    "        credentials = json.load(json_file)\n",
    "        token = eumdac.AccessToken((credentials['consumer_key'], credentials['consumer_secret']))\n",
    "        print(f\"This token '{token}' expires {token.expiration}\")\n",
    "\n",
    "# create data store object\n",
    "    datastore = eumdac.DataStore(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    #list Poseidon-4 collection IDs\n",
    "    datastore = eumdac.DataStore(token)\n",
    "    for collection_id in datastore.collections:\n",
    "        if (\"Poseidon-4\" in collection_id.title):\n",
    "            print(f\"Collection ID({collection_id}): {collection_id.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lists above, we will pick the \"Poseidon-4 Altimetry Level 2 Low Resolution (baseline version FXX) - Sentinel-6 - Reprocessed\" collection ID. (the baseline number is evolving with new reprocessing; if you have older files from another baseline (see at the end of the folder/file name, just before the .SEN6 or .nc), replace them with the newest, *_do not use inhomogeneous baselines_* in computations such as mean sea level)\n",
    "\n",
    "To download only the files we'll be using, select only the \"reduced\" (RED) files only from the NTC delay ('NT') and download it. (NB using the Reprocessed product, they will always be \"NTC\", so there is no necessity for this condition in fact - just to show how it is done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    collectionID = 'EO:EUM:DAT:0239'\n",
    "    # space/time filter the collection for products\n",
    "    selected_collection = datastore.get_collection(collectionID)\n",
    "\n",
    "    products = selected_collection.search(\n",
    "        cycle=cycle,\n",
    "        timeliness=\"NT\")\n",
    "\n",
    "    print(f\"Found {len(products)} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    for product in products:\n",
    "        for entry in product.entries:\n",
    "            if 'S6A_P4_2__LR_RED_' in entry:\n",
    "                output_file = os.path.join(os.getcwd(), 'products',os.path.basename(entry))\n",
    "                if not os.path.exists(output_file):\n",
    "                    with product.open(entry=entry) as fsrc, open(output_file, mode='wb') as fdst:\n",
    "                        print(f'Downloading {fsrc.name}.')\n",
    "                        shutil.copyfileobj(fsrc, fdst)\n",
    "                        print(f'Download of file {fsrc.name} finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data from a full cycle (track 1 to 254) from Sentinel-6 Reduced Non Time Critical level 2 Low resolution data.   \n",
    "LR are more homogeneous with previous mission (and you don't need high resolution to compute a global mean sea level). \n",
    "NTC are more accurate (in particular relative to the orbit), and more homogeneous along the mission; when a reprocessing is done, it is done on the whole mission from its beginning, while NRT or STC processing or corrections can be changed without this applied to the oldest data. \n",
    "Reduced is because we don't need the extra information you have in the standard version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading all the files together + defining needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno -101] NetCDF: HDF error: b'/Users/benloveday/Code/Git_Reps/CMTS/internal/applications/ocean-case-studies/Case_studies/UN_Ocean_Decade/Challenge07_quantifying_sealevel_rise_with_S6/products/S6A_P4_2__LR_RED__NT_053_245_20220426T193100_20220426T202713_F06.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/file_manager.py:210\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/Users/benloveday/Code/Git_Reps/CMTS/internal/applications/ocean-case-studies/Case_studies/UN_Ocean_Decade/Challenge07_quantifying_sealevel_rise_with_S6/products/S6A_P4_2__LR_RED__NT_053_245_20220426T193100_20220426T202713_F06.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '0d135c48-7a47-4084-a348-9a012498e3c9']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m s6_files\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s6_files \u001b[38;5;241m!=\u001b[39m []:\n\u001b[0;32m----> 8\u001b[0m     s6cycle01 \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms6_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnested\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno_conflicts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     s6cycle01ku \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_mfdataset(s6_files, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_01/ku\u001b[39m\u001b[38;5;124m'\u001b[39m, combine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnested\u001b[39m\u001b[38;5;124m'\u001b[39m, concat_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, compat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_conflicts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/api.py:983\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m    981\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m--> 983\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [open_(p, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m    984\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/api.py:983\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    980\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m    981\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m--> 983\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m    984\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/api.py:526\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    515\u001b[0m     decode_cf,\n\u001b[1;32m    516\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    523\u001b[0m )\n\u001b[1;32m    525\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 526\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    533\u001b[0m     backend_ds,\n\u001b[1;32m    534\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    543\u001b[0m )\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:577\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    559\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    575\u001b[0m ):\n\u001b[1;32m    576\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 577\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    376\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    377\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    379\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    380\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    381\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:329\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:385\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    386\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/file_manager.py:198\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cmts_ocean_case_studies/lib/python3.9/site-packages/xarray/backends/file_manager.py:216\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    214\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    215\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 216\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2463\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2026\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -101] NetCDF: HDF error: b'/Users/benloveday/Code/Git_Reps/CMTS/internal/applications/ocean-case-studies/Case_studies/UN_Ocean_Decade/Challenge07_quantifying_sealevel_rise_with_S6/products/S6A_P4_2__LR_RED__NT_053_245_20220426T193100_20220426T202713_F06.nc'"
     ]
    }
   ],
   "source": [
    "s6_files = glob.glob(os.path.join(download_dir,'S6A_P4_2__LR_RED_*.nc'))\n",
    "#we sort the files - by default it is \"alphabetical\" order, but pass numbers are also correctly ordered, which is our goal\n",
    "# this sorting will provide with a well-orderer dataset/nc file (see next cell). \n",
    "# It is not absolutely vital to the present computation, but could be necessary in other cases\n",
    "s6_files.sort()\n",
    "\n",
    "if s6_files != []:\n",
    "    s6cycle01 = xr.open_mfdataset(s6_files, group='data_01', combine='nested', concat_dim='time', compat='no_conflicts')\n",
    "    s6cycle01ku = xr.open_mfdataset(s6_files, group='data_01/ku', combine='nested', concat_dim='time', compat='no_conflicts')\n",
    "else:\n",
    "    print('no files found')\n",
    "\n",
    "#definition of the variables we will need\n",
    "lat = s6cycle01['latitude']\n",
    "lon = s6cycle01['longitude']\n",
    "time = s6cycle01['time']\n",
    "ssha = s6cycle01ku['ssha']\n",
    "\n",
    "# use of the different flags ; 0 is for ocean (surface flag), no rain nor sea ice or for \"good\" (quality flags)\n",
    "# alternate values are > 0, so if all the conditions are met, the sum will be zero.\n",
    "flags_ssha = np.array(s6cycle01.surface_classification_flag) \\\n",
    "           + np.array(s6cycle01ku.range_ocean_qual) \\\n",
    "           + np.array(s6cycle01ku.off_nadir_angle_wf_ocean_qual) \\\n",
    "           + np.array(s6cycle01ku.sig0_ocean_qual) \\\n",
    "           + np.array(s6cycle01ku.swh_ocean_qual) \\\n",
    "           + np.array(s6cycle01.rain_flag) \\\n",
    "           + np.array(s6cycle01.meteo_map_availability_flag) \\\n",
    "           + np.array(s6cycle01.mean_sea_surface_sol1_qual) \\\n",
    "           + np.array(s6cycle01.rad_sea_ice_flag) \\\n",
    "           + np.array(s6cycle01.rad_wet_tropo_cor_qual)\n",
    "ssha[flags_ssha != 0.0] = np.nan\n",
    "\n",
    "# remove a few extreme outliers left\n",
    "ssha = ssha.where((ssha > -2) & (ssha < 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s6cycle01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the full cycle ; at this stage, we have along-track data with time as dimension. \n",
    "Note that this plot is taking time with 254 times about 3350 points to plot (**_this cell can be skipped with no impact on the following computations_**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(), aspect=1.25)\n",
    "ax.set_extent([-180, 180, -90, 90], crs=ccrs.PlateCarree())\n",
    "#zoom on the Mediterranean Western basin\n",
    "#ax.set_extent([0, 25, 30, 50], crs=ccrs.PlateCarree())\n",
    "ax.add_feature(cartopy.feature.LAND, linewidth=1, facecolor='lightgrey', edgecolor='k', zorder=0)\n",
    "# actually plot the data\n",
    "tracks = ax.scatter(lon, lat, c=ssha, cmap='RdYlBu_r', marker='o',  vmin=-0.25, vmax=0.25, edgecolors='black', linewidth=0.01)\n",
    "cbar = plt.colorbar(tracks)\n",
    "plt.title('Sea surface height with respect to a mean sea surface (SSHA) [m]', size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the datasets from the two groups in one, extracting only ssha (which is in the Ku group), and the coordinates/dimension (which are in the group above). We define the attributes which can be needed for an output NetCDF, so as to be able to use it as such afterwards (not necessary if you only want to compute the mean of the cycle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating a proper dataset with useful attributes for a nc files \n",
    "ds = xr.Dataset(\n",
    "    {'ssha01': (['time'],  \\\n",
    "                ssha.data, \\\n",
    "                {'standard_name' : 'sea_surface_height_above_sea_level', \\\n",
    "                 'units' : \"m\", \\\n",
    "                 'coordinates' : \"longitude latitude\"})}, \n",
    "    coords={\n",
    "        'time': (['time'], \\\n",
    "                 time.data, \\\n",
    "                 {'standard_name' : 'time', \\\n",
    "                  #'units' : \"seconds since 2000-01-01 00:00:00.0\", \\ # a unit is kept in the concatenation by this dimension\n",
    "                  'coordinates' : \"longitude latitude\"}),\n",
    "        'lon01': (['time'], lon.data, \\\n",
    "                {'standard_name' : 'longitude', \\\n",
    "                 'units' : \"degrees_east\", \\\n",
    "                 'coordinates' : \"longitude latitude\"}), \n",
    "        'lat01': (['time'], lat.data, \\\n",
    "                  {'standard_name' : 'latitude', \\\n",
    "                   'units' : \"degrees_north\", \\\n",
    "                   'coordinates' : \"longitude latitude\"})})\n",
    "\n",
    "# a few time steps are duplicated at the end of some passes and at the beggining of the following one. We are removing them.\n",
    "## solution with an xarray version > 2022, to be tested \n",
    "#ds = ds.drop_duplicates(dim=\"time\",  keep='first')\n",
    "# solution with xarray versions before 2022: selection only non (~) duplicated times\n",
    "ds = ds.sel(time=~ds.indexes['time'].duplicated())\n",
    "\n",
    "ds.to_netcdf('sshafullcycle_'+str(cycle)+'.nc')\n",
    "s6cycle01.close()\n",
    "s6cycle01ku.close()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are \"binning\" the data in the two spatial dimensions, i.e. averaging them over boxes in longitude/latitude ; we do not take into account values at NaN (any arithmetic operation with a NaN in will result in NaN: we want to ignore those values when they are only part of the pixel - not when a pixel is always at NaN). However, note that due to the size of the pixels we are defining, a lot of pixels will be in part over lands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps in degrees for the pixels we will create. \n",
    "# do not go much under those values, due to the gap between tracks in longitude (315 km at the Equator)\n",
    "steplon = 3\n",
    "steplat = 1\n",
    "lat_min = -90. \n",
    "lon_min = 0.\n",
    "lat_max = 90; lon_max = 360 \n",
    "\n",
    "#creating the arrays of longitudes and latitudes\n",
    "# NB. last point is not within the array in Python.\n",
    "binnedlon = np.arange(lon_min,lon_max+steplon,steplon)\n",
    "binnedlat = np.arange(lat_min,lat_max+steplat,steplat)\n",
    "test = stats.binned_statistic_2d(ds.lon01, ds.lat01, ds.ssha01, statistic=np.nanmean,  bins=[binnedlon,binnedlat])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binned_statistic-2d is outputing a (x,y) ndarray. However, in particular to plot the data on a map, you'll need a (lat,lon) matrix, so you have to \"transpose\" your ndarray (i.e. to switch dimensions).\n",
    "\n",
    "Moreover, not to miss the 0 - 360 transition, we added one point to the statistics. \n",
    "We are choosing to reference the pixels (\"registration\") at their center (\"pixel registration\"), so we shift by half a pixel the lon/lat defining each pixel. The other usual solution is top left (\"gridline registration\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#defining an output xarray dataset\n",
    "binnedgrid = xr.Dataset({\n",
    "    'binnedssha': xr.DataArray(\n",
    "        data = test.statistic.T,\n",
    "        dims = ['latitude','longitude'],\n",
    "        coords =  {'latitude': test.y_edge[:-1]+ steplat/2, 'longitude': test.x_edge[:-1] + steplon/2},\n",
    "        attrs = {'long_name': 'sea surface heights anomalies by lat/lon bins', 'units': 'm', 'standard_name': 'sea_surface_height_above_sea_level'}\n",
    "        )\n",
    "    },\n",
    "    #global attributes can be added\n",
    "    attrs = { }\n",
    "    )\n",
    "binnedgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the output binned grid in a netCDF file\n",
    "binnedgrid.to_netcdf('binnedgrid.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the resulting grid. It is a very pixellized map - which was the objective.\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=0.))\n",
    "# overlaying the continents on top makes a nicer plot (zorder continent > zorder SSHA).\n",
    "# ax.add_feature(cartopy.feature.LAND, linewidth=1, facecolor='lightgrey', edgecolor='k', zorder=1)\n",
    "ax.add_feature(cartopy.feature.LAND, linewidth=1, facecolor='lightgrey', edgecolor='k')\n",
    "ax.set_extent([-180, 180, lat_min, lat_max])\n",
    "mymap = ax.pcolor(binnedgrid.longitude, binnedgrid.latitude, binnedgrid.binnedssha, cmap='RdYlBu_r', vmin=-0.15, vmax=0.15, shading=None, zorder=0)\n",
    "# Add a colorbar to the map object to give scale\n",
    "plt.colorbar(mymap, ax=ax)\n",
    "# Add a title to the map object\n",
    "plt.title('binned Sea surface height with respect to a mean sea surface (SSHA) [m]', size=18)\n",
    "plt.show()                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the global mean for the cycle. The area of each pixel is different depending on the latitude, \n",
    "so we weight our pixels with cos(latitude) so as not to overestimate the high latitude contributions.\n",
    "\n",
    "One more refinement would be to weight each binned pixel with the amount of ocean within (i.e. not count land in the whole pixel \"weight\"; here we overweight coastal areas in the computation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.cos(np.deg2rad(binnedgrid['latitude']))\n",
    "weights.name = \"weights\"\n",
    "ssha_weighted = binnedgrid['binnedssha'].weighted(weights)\n",
    "ssha_weighted\n",
    "weighted_mean = ssha_weighted.mean((\"longitude\", \"latitude\"))\n",
    "print(\"the Global mean sea level value (with respect to a 20-year mean) for this cycle is: \", np.around(weighted_mean.values,4), \" meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an exercice to show how it is done. \n",
    "\n",
    "It is not, however, the whole processing to do to output a complete 30-year long time series showing sea level long-term changes and trend: once done on every single cycle (since Jan. 1993! - use harmonized data with homogeneous corrections and references). To have an interannual trend you need to remove seasonal cycles, filter for 60-day and six-month periodic variations. You also have intermission biases to apply when switching from one satellite to its follow-on. You can also (or not) remove a bias to take the Glacial Isostatic Adjustement into account (-0.3 mm/yr). See https://www.aviso.altimetry.fr/en/data/products/ocean-indicators-products/mean-sea-level/processing-and-corrections.html for the details of the complete computation of a mean sea level time series from the reference mission.\n",
    "\n",
    "Beware that the zero level of this computation is *_not_* the one you can usually see on altimetry-derived mean sea level curves : here zero is set by construction at the 20-year mean, while most usually the zero is set at the level of the beginning of the series (in 1993)."
   ]
  }
 ],
 "metadata": {
  "author": "Ben Loveday, Hayley Evers-King, Vinca Rosmorduc",
  "description": "This Notebook shows how to calculate mean seal level from Sentinel-6 full cycle",
  "image": "../../../img/thumbs/Quantifying_sea_level_rise_with_S6_thumb.png",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "services": {
   "eumetsat": {
    "binder": {
     "link": "https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.eumetsat.int%2Feumetlab%2Foceans%2Focean-training%2Fapplications%2ocean-case-studies/HEAD?urlpath=%2Ftree%2FCase_studies%2FUN_Ocean_Decade%2FChallenge07_expanding_the_GOOS%2FQuantifying_sea_level_rise_with_S6%2FQuantifying_sea_level_rise_with_S6.ipynb",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    },
    "git": {
     "link": "https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/applications/ocean-case-studies/-/blob/main/Case_studies/UN_Ocean_Decade/Challenge07_expanding_the_GOOS/Quantifying_sea_level_rise_with_S6/Quantifying_sea_level_rise_with_S6.ipynb",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    }
   },
   "wekeo": {
    "git": {
     "link": "",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    },
    "url": {
     "link": "",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    }
   }
  },
  "tags": {
   "domain": "Marine",
   "platform": [
    "Sentinel-6"
   ],
   "sensor": ["Poseidon-4"],
   "service": "EUMETSAT",
   "subtheme": "Maritime safety",
   "tags": "Sea surface height"
  },
  "title": "Measuring sea level rise with Sentinel-6"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
