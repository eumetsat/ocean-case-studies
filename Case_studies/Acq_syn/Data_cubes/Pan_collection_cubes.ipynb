{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/tools/frameworks/-/raw/main/img/Standard_banner.png' align='right' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../../../Index.ipynb\"><< Index</a>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#138D75\">**Copernicus Marine Training Service**</font> <br>\n",
    "**Copyright:** 2024 European Union <br>\n",
    "**License:** MIT <br>\n",
    "**Authors:** Ben Loveday (EUMETSAT/Innoflair UG), Hayley Evers-King (EUMETSAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "   <div style=\"float:left\"><p>&emsp;</p></div>\n",
    "   <div style=\"float:left\"><a href=\"https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/applications/ocean-case-studies\"><img src=\"https://img.shields.io/badge/open-EUMETLAB-E67E22.svg\"></a></div>\n",
    "   <div style=\"float:left\"><p>&emsp;</p></div>\n",
    "   <div style=\"float:left\"><a href=\"https://user.eumetsat.int/data/themes/marine\"><img src=\"https://img.shields.io/badge/open-USER PORTAL-154360.svg\"></a></div>\n",
    "   <div style=\"float:left\"><p>&emsp;</p></div>\n",
    "   <div style=\"float:left\"><a href=\"https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.eumetsat.int%2Feumetlab%2Foceans%2Focean-training%2Fapplications%2Focean-case-studies/HEAD?labpath=Case_studies%2FAcq_syn%2FData_cubes%2FPan_collection_cubes.ipynb\"><img src=\"https://mybinder.org/badge_logo.svg\"></a></div>\n",
    "   <div style=\"float:left\"><p>&emsp;</p></div>\n",
    "   <div style=\"float:left\"><a href=\"https://jupyterhub.prod.wekeo2.eu/hub/user-redirect/lab/tree/public/wekeo4oceans/ocean-case-studies/Case_studies/Acq_syn/Data_cubes/Pan_collection_cubes.ipynb\"><img src=\"https://img.shields.io/badge/launch-WEKEO-1a4696.svg\"></a></div>\n",
    "   <div style=\"float:left\"><p>&emsp;</p></div>\n",
    "   <div style=\"float:left\"><a href=\"https://code.insula.destine.eu/hub/\"><img src=\"https://img.shields.io/badge/launch-DestinE-f43fd3.svg\"></a></div></div>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<h3>Ocean case studies</h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b>PREREQUISITES </b>\n",
    "\n",
    "This notebook has the following prerequisites:\n",
    "- **<a href=\"https://eoportal.eumetsat.int/\" target=\"_blank\">A EUMETSAT Earth Observation Portal account</a>** to download from the EUMETSAT Data Store\n",
    "\n",
    "There are no prerequisite notebooks for this module, but you may wish to investigate the following; <br>\n",
    "- The **<a href=\"https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/sensors/learn-olci/-/blob/main/2_OLCI_advanced/2_2b_OLCI_ope_rep_time_series.ipynb\" target=\"_blank\">OLCI time series</a>** Jupyter Notebook (EUMETSAT Gitlab)\n",
    "- The **<a href=\"https://gitlab.eumetsat.int/eumetlab/data-services/eumdac_data_tailor/-/blob/master/1_Using_the_Data_Tailor_with_EUMDAC.ipynb\" target=\"_blank\">Using the Data Tailor with eumdac</a>** Jupyter Notebook (EUMETSAT Gitlab). \n",
    "\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating pan-collection times series and data cubes with the EUMETSAT Data Tailor\n",
    "\n",
    "### Data used\n",
    "\n",
    "| Dataset | EUMETSAT Data Store<br>collection ID| EUMETSAT collection<br>description | WEkEO dataset ID | WEkEO description |\n",
    "|:--------------------:|:-----------------------:|:-------------:|:-----------------:|:-----------------:|\n",
    "| Sentinel-3 OLCI level-2 full resolution | EO:EUM:DAT:0407 | <a href=\"https://user.eumetsat.int/catalogue/EO:EUM:DAT:SENTINEL-3:OL_2_WFR___NTC\" target=\"_blank\">Description</a> | EO:EUM:DAT:SENTINEL-3:OL_2_WFR___ | <a href=\"https://www.wekeo.eu/data?view=dataset&dataset=EO%3AEUM%3ADAT%3ASENTINEL-3%3AOL_2_WFR___\" target=\"_blank\">Description</a> |\n",
    "| Sentinel-3 OLCI level-2 full resolution **reprocessed** (BC003) | EO:EUM:DAT:0556 | <a href=\"https://user.eumetsat.int/catalogue/EO:EUM:DAT:0556\" target=\"_blank\">Description</a> | EO:EUM:DAT:SENTINEL-3:0556 | <a href=\"https://www.wekeo.eu/data?view=dataset&dataset=EO%3AEUM%3ADAT%3ASENTINEL-3%3A0556\" target=\"_blank\">Description</a> |\n",
    "\n",
    "\n",
    "### Learning outcomes\n",
    "\n",
    "At the end of this notebook you will know how to;\n",
    "* Differentiate between reprocessed and operational data\n",
    "* Construct searches for \"best quality\" time series across collections\n",
    "* Download and tailor your products of interest\n",
    "* Create and visualise a small data cube\n",
    "\n",
    "\n",
    "### Outline\n",
    "\n",
    "<center><img src='https://www.ismar.cnr.it/wp-content/uploads/2023/07/PTF-2018-05-Pomaro-1-scaled-1.jpg' align='center' width='33%'/><br><br><b>Figure 1. The Acqua Alta oceanographic tower. Image Credit: CNR-ISMAR</a></b></img><br><br></center>\n",
    "\n",
    "For different applications it can be necessary combine multiple data sources and transform them to allow for certain types of analysis. To support this, EUMETSAT has developed a Data Tailor which supports to extraction of regions of interest, as well as geospatial and format transformations. This notebook will show how to search for a region of interest across **operational** and **reprocessed** data collections in the EUMETSAT Data Store. A list of results is then passed to the online Data Tailor web service to extract a subregion and reproject the resulting data on to a consistent projection. The end result is a small data cube which is visualised using the xcube viewer tool. \n",
    "\n",
    "This workflow could be used for different products available from the EUMETSAT Data Store, where they are supported by the Data Tailor. In the example here we focus on Sentinel-3 OLCI level-2 ocean colour data, around a site of interest to the ocean colour community - the Acqua Alta tower.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='TOCTOP'></a>Contents\n",
    "\n",
    "</div>\n",
    "\n",
    " 1. [Step 1: Setting up our analysis](#section1)\n",
    " 1. [Step 2: Authenticating the EUMETSAT Data Store](#section2)\n",
    " 1. [Step 3: Selecting data from the EUMETSAT Data Store](#section3)\n",
    " 1. [Step 4: Filtering the operational collection by time](#section4)\n",
    " 1. [Step 5: Constructing our time series](#section5)\n",
    " 1. [Step 6: Tailoring and downloading products](#section6)\n",
    " 1. [Step 7: Viewing our tailored products](#section7)\n",
    " 1. [Step 8: Conclusions](#section8)\n",
    " 1. [Step 9: Challenges (optional)](#section9)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section1'></a>1. Setting up our analysis\n",
    "[Back to top](#TOCTOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing all of the libraries that we need to run this notebook. If you have built your python using the environment file provided in this repository, then you should have everything you need. For more information on building environment, please see the repository **<a href=\"../../../README.md\" target=\"_blank\">README</a>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime                           # a library that allows us to work with dates and times\n",
    "import eumdac                             # a tool that helps us download via the eumetsat/data-store\n",
    "import fnmatch                            # a library that allows us to filter for file types\n",
    "import getpass                            # a library to help us enter passwords\n",
    "import glob                               # a library that aids in searching for files\n",
    "import matplotlib.pyplot as plt           # a library that support plotting\n",
    "import os                                 # a library that allows us access to basic operating system commands\n",
    "from pathlib import Path                  # a library to help us to construct system paths\n",
    "import shutil                             # a library that allows us access to basic operating system commands like copy\n",
    "import time                               # a library that helps us manage script timing\n",
    "import warnings                           # a library that helps us handle warnings\n",
    "import xarray as xr                       # a library that supports the use of multi-dimensional arrays in Python\n",
    "from xcube.webapi.viewer import Viewer    # a library that provides the Xcube viewer\n",
    "\n",
    "# turn off any script warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by setting a few parameters that we will need for our experiment, beginning with the location of the Acqua Alta Oceanographic Tower and the size of the box we want to acquire around this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acqua Alta location\n",
    "lat = 45.31435\n",
    "lon = 12.508317\n",
    "\n",
    "# size of the box we want to acquire around the location of interest\n",
    "spatial_tolerance = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets specify the collections of interest that contain our **operational** and **reprocessed** data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectionID_OPE = \"EO:EUM:DAT:0407\"\n",
    "collectionID_REP = \"EO:EUM:DAT:0556\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from Sentinel-3 is available at various timeliness. Near real-time data (NR) is available quickly, but does not have the highest quality. Non time-critical data (NT) is available more slowly, but is higher quality. All reprocessed data is non time-critical. We never want to mix reprocessed and near real-time data, so we will limit ourselves to NT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeliness = \"NT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, for the sake of our demo, we will limit the scenes we process in the final step. You should feel free to customise these settings as you see fir for your own analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the first scene to process\n",
    "scene_start = 17\n",
    "\n",
    "# set the number of scenes from the first scene to process\n",
    "nscenes = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first instance of running this script, we want to download data, but in subsequent runs we may want to use what we have. The `download_data` switch below will tell us to download data (when `True`) or not (when `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to begin gathering our data for analysis, beginning by authenticating our access to the EUMETSAT Data Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section2'></a>2. Authenticating the EUMETSAT Data Store\n",
    "[Back to top](#TOCTOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Accessing the EUMETSAT Data Store\n",
    "\n",
    "To access Copernicus marine data from the <a href=\"https://data.eumetsat.int \" target=\"_blank\">EUMETSAT Data Store</a>, we will use the EUMETSAT Data Access Client (`eumdac`) python package. If you are working with the recommended Anaconda Python distribution and used the environment file included in this repository (environment.yml) to build this python environment (as detailed in the README), you will already have installed this. If not, you can install eumdac using;\n",
    "\n",
    "`conda install -c eumetsat eumdac`\n",
    "\n",
    "You can also find the source code on the <a href=\"https://gitlab.eumetsat.int/eumetlab/data-services/eumdac \" target=\"_blank\">EUMETSAT GitLab</a>. Please visit the EUMETSAT user portal for more information on the <a href=\"https://user.eumetsat.int/data-access/data-store \" target=\"_blank\">EUMETSAT Data Store</a> and <a href=\"https://user.eumetsat.int/resources/user-guides/eumetsat-data-access-client-eumdac-guide \" target=\"_blank\">eumdac</a>.\n",
    "\n",
    "To download data from the EUMETSDAT Data Store via API, you need to provide credentials. To obtain these you should first register at for an <a href=\"https://eoportal.eumetsat.int/\" target=\"_blank\">EUMETSAT Earth Observation Portal account</a>. Once you have an account, you can retrieve your `<your_consumer_key>` and `<your_consumer_secret>` from the <a href=\"https://api.eumetsat.int/api-key/ \" target=\"_blank\">\"EUMETSAT Data Store API\"</a> page (*Note: you must click the \"Show hidden fields\" button at the bottom of the page to see the relevant fields*). If you do not already have a local credentials file, you will be prompted to enter your credentials when you run the cell below. This will create the required local credentials file, so that you only need to run this once.\n",
    "\n",
    "*Note: your key and secret are permanent, so you should take care to never share them*\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This token '51b94db4-b0fc-3019-b8d3-6f4c5caa7436' expires 2024-10-25 13:01:15.326345\n"
     ]
    }
   ],
   "source": [
    "# load credentials\n",
    "eumdac_credentials_file = Path(Path.home() / '.eumdac' / 'credentials')\n",
    "\n",
    "if os.path.exists(eumdac_credentials_file):\n",
    "    consumer_key, consumer_secret = Path(eumdac_credentials_file).read_text().split(',')\n",
    "else:\n",
    "    # creating authentication file\n",
    "    consumer_key = input('Enter your consumer key: ')\n",
    "    consumer_secret = getpass.getpass('Enter your consumer secret: ')\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(eumdac_credentials_file), exist_ok=True)\n",
    "        with open(eumdac_credentials_file, \"w\") as f:\n",
    "            f.write(f'{consumer_key},{consumer_secret}')\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "token = eumdac.AccessToken((consumer_key, consumer_secret))\n",
    "print(f\"This token '{token}' expires {token.expiration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a token, we can create an instance of the EUMETSAT Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = eumdac.DataStore(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section3'></a>3. Selecting data from the EUMETSAT Data Store\n",
    "[Back to top](#TOCTOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have authenticated our `datastore` object, lets connect to the two collections we are interested in using the `get_collection` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use collection ID\n",
    "collection_OPE = datastore.get_collection(collectionID_OPE)\n",
    "collection_REP = datastore.get_collection(collectionID_REP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get information on each collections using the checking the `abstract` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLCI Level 2 Ocean Colour Full Resolution - Sentinel-3\n",
      "---\n",
      "OLCI (Ocean and Land Colour Instrument) Ocean Colour Geophysical Products. Full Resolution: 300m at nadir. All Sentinel-3 NRT products are available at pick-up point in less than 3h. Level 2 marine products include the following: * water-leaving reflectances in 16 bands, Oa**_reflectance (Baseline Atmospheric Correction (BAC) algorithm, bands: 400, 412, 442, 490, 510, 560, 620, 665, 674, 681, 709, 754, 779, 865, 885, 1024 nanometer (nm)); *algal pigment concentration in clear waters, chl_oc4me (BAC and maximum band ratio algorithm, log10 scaled); *algal pigment concentration in turbid waters, chl_nn (neural net algorithm, log10 scaled); *total suspended matter concentration, tsm_nn (neural net algorithm, log10 scaled); *diffuse attenuation coefficient Kd of downward irradiance at 490 nm, trsp (BAC and M07 algorithm, log10 scaled); *absorption coefficient at 443 nm of coloured detrital and dissolved organic matter, iop_nn (neural net algorithm, log10 scaled); *instantaneous photosynthetically active radiation, PAR (BAC and clear-sky ocean algorithm); *aerosol optical thickness T865 and aerosol Angstrom exponent A865, w_aer (BAC algorithm, A for bands 779 and 865 nm) *integrated water vapour column, iwv (1D-Var algorithm). The geophysical products are accompanied by error estimate products. Pixel classification, quality and science flags, as well as meteorological, geometry and geolocation data at tie points are provided. - All Sentinel-3 NRT products are available at pick-up point in less than 3h - All Sentinel-3 Non Time Critical (NTC) products are available at pick-up point in less than 30 days Sentinel-3 is part of a series of Sentinel satellites, under the umbrella of the EU Copernicus programme.\n",
      "\n",
      "OLCI Level 2 Ocean Colour Full Resolution (version BC003) - Sentinel-3 - Reprocessed\n",
      "---\n",
      "OLCI Level 2 Marine products provide spectral information on the colour of the oceans (water reflectances). These radiometric products are used to estimate geophysical parameters e.g. estimates of phytoplankton biomass through determining the Chlorophyll-a (Chl) concentration. In coastal areas, they also allow monitoring of the sediment load via the Total Suspended Matter (TSM) product. Full resolution products are at a nominal 300m resolution. This collection contains reprocessed data from baseline collection 003. Operational data can be found in the corresponding collection.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for collection in [collection_OPE, collection_REP]:\n",
    "    print(f\"{collection.title}\\n---\\n{collection.abstract}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now search in each of the two collections. Want to make the longest time series possible, do we are only going to filter our search by `timeliness` and our region of interest. We can do this as follows;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a very small box around our proposed location\n",
    "ROI = [[lon - spatial_tolerance, lat - spatial_tolerance],\n",
    "       [lon - spatial_tolerance, lat + spatial_tolerance],\n",
    "       [lon + spatial_tolerance, lat + spatial_tolerance],\n",
    "       [lon + spatial_tolerance, lat - spatial_tolerance],\n",
    "       [lon - spatial_tolerance, lat - spatial_tolerance]]\n",
    "\n",
    "# convert this to a WKT polygon\n",
    "polygon = 'POLYGON(({}))'.format(','.join([\"{} {}\".format(*coord) for coord in ROI]))\n",
    "\n",
    "# search for our products\n",
    "products_OPE = collection_OPE.search(geo=polygon, timeliness=timeliness)\n",
    "products_REP = collection_REP.search(geo=polygon, timeliness=timeliness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how any products these searches yield...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2039 reprocessed products\n",
      "Found 1795 operational products\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(products_REP)} reprocessed products\")\n",
    "print(f\"Found {len(products_OPE)} operational products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section4'></a>Step 4: Filtering the operational collection by time\n",
    "[Back to top](#TOC-TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our reprocessing and operational records overlap we only wish to retain the former. Put another way, we only want to include the operational products that were created after the end of the reprocessing was completed. We can do this by filtering the operational collection by date, limiting it to dates after the last available reprocessed products to now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last available reprocessing products is at 20210428T094703\n",
      "Found 1795 matching products that occur after the end of the reprocessing\n"
     ]
    }
   ],
   "source": [
    "latest = products_REP.first()\n",
    "file_tags = str(latest).split('_')\n",
    "file_tags = [i for i in file_tags if i]\n",
    "latest_date = file_tags[4]\n",
    "print(f\"The last available reprocessing products is at {latest_date}\")\n",
    "\n",
    "products_OPE = collection_OPE.search(geo=polygon,\n",
    "                                     dtstart=datetime.datetime.strptime(latest_date, \"%Y%m%dT%H%M%S\") + datetime.timedelta(seconds=1),\n",
    "                                     timeliness=timeliness)\n",
    "\n",
    "print(f\"Found {len(products_OPE)} matching products that occur after the end of the reprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section5'></a>Step 5: Constructing our time series\n",
    "[Back to top](#TOC-TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two sets of matching products, one reprocessed and one operational that begins once the reprocessed record is complete. Lets combine these into one list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we convert our REP search results to a product list so that we can concatenate the two records\n",
    "final_products_REP = [item for item in products_REP]\n",
    "final_products_OPE = [item for item in products_OPE]\n",
    "\n",
    "# then we concatenate the lists\n",
    "final_products = final_products_OPE + final_products_REP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..and do a quick check on our data coverage to make sure that the reprocessed and operational collections match nicely at the join of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last available reprocessed product is:\n",
      "S3A_OL_2_WFR____20210428T094703_20210428T095003_20220308T095437_0179_071_136______MAR_R_NT_003.SEN3\n",
      "The first available operational product is:\n",
      "S3A_OL_2_WFR____20210429T092052_20210429T092352_20210430T193628_0179_071_150_2160_MAR_O_NT_003.SEN3\n"
     ]
    }
   ],
   "source": [
    "print(f\"The last available reprocessed product is:\\n{str(final_products_REP[0])}\")\n",
    "print(f\"The first available operational product is:\\n{str(final_products_OPE[-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good! We are read to get the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section6'></a>Step 6: Tailoring and downloading products\n",
    "[Back to top](#TOC-TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could, at this stage, download the data directly, but as we want to make a data cube we need to take advantage of another EUMETSAT service; the Data Tailor Web Service. The Data Tailor Web Service allows us to perform remote processing on the items we searched for. You can find more information on its usage on our <a href=\"https://user.eumetsat.int/resources/user-guides/eumetsat-data-access-client-eumdac-guide#ID-Data-Tailor\" target=\"_blank\">EUMDAC User Portal</a> pages. We can instantiate the Data Tailor as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatailor = eumdac.DataTailor(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a number of processes to perform, and we can stage these in one \"chain\" configuration. We will add the following steps;\n",
    "\n",
    "* `roi`: The data are large, and we may not need to work with entire granules. We can use the `roi` option to subset our data.\n",
    "* `filter`: We also do not need to work with every variable. In our case, we only care about the neural network chlorophyll concentration, and we can select this from the available \"bands\" as required.\n",
    "* `projection`: Level-2 OLCI data is made available on the instrument grid, meaning the the granules that cover a specified point do no exactly overlap. We need to transform the data to a common grid, and we can use reprojection to do this via the `projection` option.\n",
    "* `resample_resolution`: we need to set the output resolution of our projected product in degrees. We should set this to as close to the native resolution of the sensor as possible, but not exceed it. For OLCI full resolution 300 m products 0.003 is a suitable number.\n",
    "* `format`: We want to ensure that we get output in netCDF\n",
    "\n",
    "*Note that the `product` keyword is determined by the data we are working with, in thicase OLCI Level-2 full resolution products (OLL2WFR).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the chain configuration\n",
    "chain = eumdac.tailor_models.Chain(\n",
    "    product='OLL2WFR',\n",
    "    roi={\"NSWE\" : [lat+spatial_tolerance, lat-spatial_tolerance, lon-spatial_tolerance, lon+spatial_tolerance]},\n",
    "    filter={\"bands\" : [\"chl_nn\"]},\n",
    "    projection='geographic',\n",
    "    resample_resolution=[0.003, 0.003],\n",
    "    format='netcdf4'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets select some of our products to customise with out Data Tailor chain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few products for our example\n",
    "selected_products = final_products[scene_start:scene_start+nscenes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets iterate through our selected products, feeding them into the Data Tailor one at a time. Each will be customised according to our processing chain and downloaded when done. The cell below will monitor the remote service to see if the customisation is successful or not. You can always manually check what is running by going to <a href=\"https://tailor.eumetsat.int/launchpad\" target=\"_blank\">https://tailor.eumetsat.int/launchpad</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customising S3A_OL_2_WFR____20241010T095428_20241010T095728_20241011T160342_0179_118_022_2160_MAR_O_NT_003.SEN3\n",
      "Customisation 29306128 is running.\n",
      "Customisation 29306128 is running.\n",
      "Customisation 29306128 is running.\n",
      "Customisation 29306128 is successfully completed.\n",
      "Customisation 29306128 outputs downloading.\n",
      "Customisation 29306128 outputs cleared.\n",
      "Customising S3B_OL_2_WFR____20241010T091536_20241010T091836_20241011T161951_0179_098_264_2160_MAR_O_NT_003.SEN3\n",
      "Customisation e996a33a is running.\n",
      "Customisation e996a33a is running.\n",
      "Customisation e996a33a is successfully completed.\n",
      "Customisation e996a33a outputs downloading.\n",
      "Customisation e996a33a outputs cleared.\n",
      "Customising S3B_OL_2_WFR____20241009T094147_20241009T094447_20241010T170335_0180_098_250_2160_MAR_O_NT_003.SEN3\n",
      "Customisation a4a70397 is running.\n",
      "Customisation a4a70397 is running.\n",
      "Customisation a4a70397 is successfully completed.\n",
      "Customisation a4a70397 outputs downloading.\n",
      "Customisation a4a70397 outputs cleared.\n",
      "Customising S3A_OL_2_WFR____20241008T090549_20241008T090849_20241009T154830_0179_117_378_2160_MAR_O_NT_003.SEN3\n",
      "Customisation 906a5022 is running.\n",
      "Customisation 906a5022 is running.\n",
      "Customisation 906a5022 is successfully completed.\n",
      "Customisation 906a5022 outputs downloading.\n",
      "Customisation 906a5022 outputs cleared.\n",
      "Customising S3A_OL_2_WFR____20241007T093200_20241007T093500_20241008T163111_0180_117_364_2160_MAR_O_NT_003.SEN3\n",
      "Customisation 46210aba is running.\n",
      "Customisation 46210aba is running.\n",
      "Customisation 46210aba is successfully completed.\n",
      "Customisation 46210aba outputs downloading.\n",
      "Customisation 46210aba outputs cleared.\n",
      "Customising S3B_OL_2_WFR____20241007T085310_20241007T085610_20241008T164840_0179_098_221_2160_MAR_O_NT_003.SEN3\n",
      "Customisation f1a1c652 is running.\n",
      "Customisation f1a1c652 is running.\n",
      "Customisation f1a1c652 is successfully completed.\n",
      "Customisation f1a1c652 outputs downloading.\n",
      "Customisation f1a1c652 outputs cleared.\n",
      "Customising S3A_OL_2_WFR____20241006T095810_20241006T100110_20241007T172816_0179_117_350_2160_MAR_O_NT_003.SEN3\n",
      "Customisation 342e6f5d is running.\n",
      "Customisation 342e6f5d is running.\n",
      "Customisation 342e6f5d is successfully completed.\n",
      "Customisation 342e6f5d outputs downloading.\n",
      "Customisation 342e6f5d outputs cleared.\n",
      "Customising S3B_OL_2_WFR____20241006T091921_20241006T092221_20241007T153854_0179_098_207_2160_MAR_O_NT_003.SEN3\n",
      "Customisation 1aa0a08c is running.\n",
      "Customisation 1aa0a08c is running.\n",
      "Customisation 1aa0a08c is running.\n",
      "Customisation 1aa0a08c is successfully completed.\n",
      "Customisation 1aa0a08c outputs downloading.\n",
      "Customisation 1aa0a08c outputs cleared.\n"
     ]
    }
   ],
   "source": [
    "if download_data:\n",
    "    \n",
    "    sleep_time = 10\n",
    "    \n",
    "    for product in selected_products:\n",
    "    \n",
    "        print(f\"Customising {product}\")\n",
    "        customisation = datatailor.new_customisation(product, chain=chain)\n",
    "        status = customisation.status\n",
    "        \n",
    "        # Monitor the customisation\n",
    "        while status:\n",
    "            # Get the status of the ongoing customisation\n",
    "            status = customisation.status\n",
    "        \n",
    "            if \"DONE\" in status:\n",
    "                print(f\"Customisation {customisation._id} is successfully completed.\")\n",
    "                break\n",
    "            elif status in [\"ERROR\",\"FAILED\",\"DELETED\",\"KILLED\",\"INACTIVE\"]:\n",
    "                print(f\"Customisation {customisation._id} was unsuccessful. Customisation log is printed.\\n\")\n",
    "                print(customisation.logfile)\n",
    "                break\n",
    "            elif \"QUEUED\" in status:\n",
    "                print(f\"Customisation {customisation._id} is queued.\")\n",
    "            elif \"RUNNING\" in status:\n",
    "                print(f\"Customisation {customisation._id} is running.\")\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "        # Download\n",
    "        print(f\"Customisation {customisation._id} outputs downloading.\")\n",
    "        nc, = fnmatch.filter(customisation.outputs, '*.nc') \n",
    "    \n",
    "        with customisation.stream_output(nc,) as stream, \\\n",
    "            open(stream.name, mode='wb') as fdst:\n",
    "            shutil.copyfileobj(stream, fdst)\n",
    "    \n",
    "        # Tidy up (if successful)\n",
    "        print(f\"Customisation {customisation._id} outputs cleared.\")\n",
    "        customisation.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section7'></a>Step 7: Viewing our tailored products\n",
    "[Back to top](#TOC-TOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a series of subset, reprojected OLCI Level-2 chlorophyll products in netCDF format. These tiles exactly overlap and so can be concatenated in time to form a \"data cube\". The products themselves do not contain a time variable, but this is encoded in the file name. Lets fetch all of our files and create a `times` variable from their names, as follows;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_files = sorted(glob.glob(\"OLL2*\"))\n",
    "times = [datetime.datetime.strptime(downloaded_file.split('_')[1],\"%Y%m%dT%H%M%SZ\") for downloaded_file in downloaded_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now open all of our files in an xarray ojbect and append a time variable using the information in our `times` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset(downloaded_files, combine='nested', concat_dim=\"time\")\n",
    "ds[\"time\"] = times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view our data cube, we are going to use the <a href=\"https://xcube.readthedocs.io/en/latest/examples/xcube_serve.html\" target=\"_blank\">XCube viewer</a>. This is part of the excellent Xcube that can be used to serve data cubes, both local and remote through various platforms. Here, we will simply launch the viewer in our notebook.\n",
    "\n",
    "To do this, we need to specify a configuration file that specified the \"Styles\" we want to use to describe our data. Please consult the documentation in the previous link for more information on how to adapt this to your purposes. In our case, we will simply specify the identifier of our variable \"CHL\", the name of the variable in `ds` that contains this information \"chl_nn\", then range (in log space) of the data, and the colormap to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 GET /viewer/config/config.json (127.0.0.1): xcube viewer has not been been configured\n",
      "404 GET /viewer/config/config.json (127.0.0.1) 2.85ms\n"
     ]
    }
   ],
   "source": [
    "viewer = Viewer(\n",
    "    server_config={\n",
    "        \"Styles\": [\n",
    "            {\n",
    "                \"Identifier\": \"CHL\",\n",
    "                \"ColorMappings\": {\n",
    "                    \"chl_nn\": {\"ValueRange\": [-2, 2], \"ColorBar\": \"viridis\"}\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now add this data set to our viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_dataset(ds, style=\"CHL\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets show the viewer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://localhost:8001/viewer/?serverUrl=http://localhost:8001&compact=1\" width=\"100%\" height=\"800\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "## <a id='section8'></a>8. Conclusions\n",
    "[Back to top](#TOCTOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the workflow above you should understand how to create long time series from a combination of operational and reprocessed collections available in the EUMETSAT Data Store. Using the Data Tailor you should be able to extract a region of interest and reproject data from individual files to create a data cube. Tools such as the xcube viewer support quick visualisation and exploration of these data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "\n",
    "## <a id='section9'></a>9. Challenge\n",
    "[Back to top](#TOCTOP)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have learned how to create your own data cubes, there are many other variations of this workflow you could explore.\n",
    "- Try and extract a different variable from the OLCI level-2 data e.g. TSM_NN\n",
    "- Set up a search and download data from another data collection e.g. SST - you will have to adapt how the data is read, and note that other collections might have much bigger files!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a href=\"../../../Index.ipynb\"><< Index</a>\n",
    "<hr>\n",
    "<a href=\"https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/applications/ocean-case-studies\">View on GitLab</a> | <a href=\"https://training.eumetsat.int/\">EUMETSAT Training</a> | <a href=mailto:ops@eumetsat.int>Contact helpdesk for support </a> | <a href=mailto:.training@eumetsat.int>Contact our training team to collaborate on and reuse this material</a></span></p>"
   ]
  }
 ],
 "metadata": {
  "author": "Ben Loveday, Hayley Evers-King",
  "content_type": "Software & code",
  "data_access": [
   "Data Store",
   "Data Tailor"
  ],
  "deployment": {
   "eumetsat": {
    "binder": {
     "link": "https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.eumetsat.int%2Feumetlab%2Foceans%2Focean-training%2Fapplications%2Focean-case-studies/HEAD?labpath=Case_studies%2FAcq_syn%2FData_cubes%2FPan_collection_cubes.ipynb",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    },
    "git": {
     "link": "https://gitlab.eumetsat.int/eumetlab/oceans/ocean-training/applications/ocean-case-studies/-/blob/main/Case_studies/Acq_syn/Data_cubes/Pan_collection_cubes.ipynb",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    }
   },
   "wekeo": {
    "git": {
     "link": "https://github.com/wekeo/ocean-case-studies/blob/main/Case_studies/Acq_syn/Data_cubes/Pan_collection_cubes.ipynb",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    },
    "url": {
     "link": "https://jupyterhub.prod.wekeo2.eu/hub/user-redirect/lab/tree/public/wekeo4oceans/ocean-case-studies/Case_studies/Acq_syn/Data_cubes/Pan_collection_cubes.ipynb",
     "service_contact": "ops@eumetsat.int",
     "service_provider": "EUMETSAT"
    }
   }
  },
  "description": "This Jupyter Notebook shows how to construct bespoke ocean colour data cubes using the EUMETSAT Data Store and Data Tailor",
  "image": "../../../../img/thumbs/Pan_collection_cubes_thumb.png",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "license": "MIT",
  "metadata_schema_version": "2.0.0",
  "originator": "EUMETSAT",
  "tags": {
   "data_provider": "EUMETSAT",
   "orbit": "LEO",
   "satellite": "Sentinel-3",
   "sensor": "OLCI (Sentinel-3)",
   "service": "Ocean colour",
   "subtheme": [
    "Ocean biogeochemistry",
    "Water quality"
   ],
   "theme": "Marine",
   "variable": [
    "Chlorophyll concentration",
    "Total suspended matter",
    "Ocean colour"
   ]
  },
  "title": "Constructing ocean colour data cubes using the EUMETSAT Data Store and Tailor",
  "version": "2.0.0",
  "version_date": "2024-09-02"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
