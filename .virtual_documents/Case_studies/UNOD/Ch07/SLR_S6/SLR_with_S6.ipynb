























import glob                            # a package that helps with file searching
import os                              # a library that allows us access to basic operating system commands
import eumdac                          # a tool that helps us download via the eumetsat/data-store
from pathlib import Path               # a library that helps construct system path objects
import json                            # a library that helps us make JSON format files
import datetime                        # a library that allows us to work with dates and times
import shutil                          # a library that allows us access to basic operating system commands like copy
import cartopy                         # a library that supports mapping and projection
import matplotlib.pyplot as plt        # a library the provides plotting capability
import xarray as xr                    # a library that helps us work efficiently with multi-dimensional arrays
import numpy as np                     # a library that lets us work with arrays; we import this with a new name "np"
from scipy import stats                # a statistical library
import copernicusmarine                 # a library to help us access CMEMS data
import warnings                        # a library that supports managing warning messages

# turn off warnings
warnings.filterwarnings("ignore")





# Create a download directory for our products
download_dir = os.path.join(os.getcwd(), "products")
os.makedirs(download_dir, exist_ok=True)

download_data = False
cycle = 53








# load credentials
eumdac_credentials_file = Path(Path.home() / '.eumdac' / 'credentials')

if os.path.exists(eumdac_credentials_file):
    consumer_key, consumer_secret = Path(eumdac_credentials_file).read_text().split(',')
else:
    # creating authentication file
    consumer_key = input('Enter your consumer key: ')
    consumer_secret = getpass.getpass('Enter your consumer secret: ')
    try:
        os.makedirs(os.path.dirname(eumdac_credentials_file), exist_ok=True)
        with open(eumdac_credentials_file, "w") as f:
            f.write(f'{consumer_key},{consumer_secret}')
    except:
        pass
        
token = eumdac.AccessToken((consumer_key, consumer_secret))
print(f"This token '{token}' expires {token.expiration}")





datastore = eumdac.DataStore(token)


if download_data:
    for collection_id in datastore.collections:
        if ("Poseidon-4" in collection_id.title):
            if "non-public" in collection_id.abstract: continue
            print(f"Collection ID({collection_id}): {collection_id.title}")





if download_data:
    collectionID = 'EO:EUM:DAT:0842'
    selected_collection = datastore.get_collection(collectionID)

    # space/time filter the collection for products
    products = selected_collection.search(cycle=cycle, timeliness="NT")
    print(f"Found {len(products)} products")


if download_data:
    count = 0
    for product in products:
        count = count + 1
        for entry in product.entries:
            if 'S6A_P4_2__LR_RED_' in entry:
                with product.open(entry=entry) as fsrc, open(os.path.join(os.getcwd(), 'products',fsrc.name), mode='wb') as fdst:
                    shutil.copyfileobj(fsrc, fdst)
                    print(f'Downloaded ({str(count).zfill(3)}:{len(products)}): {fsrc.name}')











s6_files = glob.glob(os.path.join(download_dir,'S6A_P4_2__LR_RED_*.nc'))
#we sort the files - by default it is "alphabetical" order, but pass numbers are also correctly ordered, which is our goal
# this sorting will provide with a well-orderer dataset/nc file (see next cell). 
# It is not absolutely vital to the present computation, but could be necessary in other cases
s6_files.sort()

if s6_files != []:
    s6cycle01 = xr.open_mfdataset(s6_files, group='data_01', combine='nested', concat_dim='time', compat='no_conflicts')
    s6cycle01ku = xr.open_mfdataset(s6_files, group='data_01/ku', combine='nested', concat_dim='time', compat='no_conflicts')
else:
    print('no files found')

#definition of the variables we will need
lat = s6cycle01['latitude']
lon = s6cycle01['longitude']
time = s6cycle01['time']
ssha = s6cycle01ku['ssha']

# use of the different flags ; 0 is for ocean (surface flag), no rain nor sea ice or for "good" (quality flags)
# alternate values are > 0, so if all the conditions are met, the sum will be zero.
flags_ssha = np.array(s6cycle01.surface_classification_flag) \
           + np.array(s6cycle01ku.range_ocean_qual) \
           + np.array(s6cycle01ku.off_nadir_angle_wf_ocean_qual) \
           + np.array(s6cycle01ku.sig0_ocean_qual) \
           + np.array(s6cycle01ku.swh_ocean_qual) \
           + np.array(s6cycle01.rain_flag) \
           + np.array(s6cycle01.meteo_map_availability_flag) \
           + np.array(s6cycle01.mean_sea_surface_sol1_qual) \
           + np.array(s6cycle01.rad_sea_ice_flag) \
           + np.array(s6cycle01.rad_wet_tropo_cor_qual)

ssha[flags_ssha != 0.0] = np.nan

# remove a few extreme outliers left
ssha = ssha.where((ssha > -2) & (ssha < 2))


s6cycle01





fig = plt.figure(figsize=(16,8))
ax = plt.axes(projection=cartopy.crs.PlateCarree(), aspect=1.25)
ax.set_extent([-180, 180, -90, 90], crs=cartopy.crs.PlateCarree())
#zoom on the Mediterranean Western basin
#ax.set_extent([0, 25, 30, 50], crs=cartopy.crs.PlateCarree())
ax.add_feature(cartopy.feature.LAND, linewidth=1, facecolor='lightgrey', edgecolor='k', zorder=0)
# actually plot the data
tracks = ax.scatter(lon, lat, c=ssha, cmap='RdYlBu_r', marker='o',  vmin=-0.25, vmax=0.25, edgecolors='black', linewidth=0.01)
cbar = plt.colorbar(tracks)
plt.title('Sea surface height with respect to a mean sea surface (SSHA) [m]', size=18)
plt.show()








#creating a proper dataset with useful attributes for a nc files 
ds = xr.Dataset(
    {'ssha01': (['time'],  \
                ssha.data, \
                {'standard_name' : 'sea_surface_height_above_sea_level', \
                 'units' : "m", \
                 'coordinates' : "longitude latitude"})}, 
    coords={
        'time': (['time'], \
                 time.data, \
                 {'standard_name' : 'time', \
                  #'units' : "seconds since 2000-01-01 00:00:00.0", \ # a unit is kept in the concatenation by this dimension
                  'coordinates' : "longitude latitude"}),
        'lon01': (['time'], lon.data, \
                {'standard_name' : 'longitude', \
                 'units' : "degrees_east", \
                 'coordinates' : "longitude latitude"}), 
        'lat01': (['time'], lat.data, \
                  {'standard_name' : 'latitude', \
                   'units' : "degrees_north", \
                   'coordinates' : "longitude latitude"})})

# a few time steps are duplicated at the end of some passes and at the beggining of the following one. We are removing them.
## solution with an xarray version > 2022, to be tested 
#ds = ds.drop_duplicates(dim="time",  keep='first')
# solution with xarray versions before 2022: selection only non (~) duplicated times
ds = ds.sel(time=~ds.indexes['time'].duplicated())

ds.to_netcdf('sshafullcycle_'+str(cycle)+'.nc')
s6cycle01.close()
s6cycle01ku.close()
ds





# steps in degrees for the pixels we will create. 
# do not go much under those values, due to the gap between tracks in longitude (315 km at the Equator)
steplon = 3
steplat = 1
lat_min = -90. 
lon_min = 0.
lat_max = 90; lon_max = 360 

#creating the arrays of longitudes and latitudes
# NB. last point is not within the array in Python.
binnedlon = np.arange(lon_min,lon_max+steplon,steplon)
binnedlat = np.arange(lat_min,lat_max+steplat,steplat)
test = stats.binned_statistic_2d(ds.lon01, ds.lat01, ds.ssha01, statistic=np.nanmean,  bins=[binnedlon,binnedlat])





#defining an output xarray dataset
binnedgrid = xr.Dataset({
    'binnedssha': xr.DataArray(
        data = test.statistic.T,
        dims = ['latitude','longitude'],
        coords =  {'latitude': test.y_edge[:-1]+ steplat/2, 'longitude': test.x_edge[:-1] + steplon/2},
        attrs = {'long_name': 'sea surface heights anomalies by lat/lon bins',
                 'units': 'm',
                 'standard_name':
                 'sea_surface_height_above_sea_level'}
        )
    },
    #global attributes can be added
    attrs = { }
    )
binnedgrid


# saving the output binned grid in a netCDF file
binnedgrid.to_netcdf('binnedgrid.nc')


# plotting the resulting grid. It is a very pixellized map - which was the objective.

plt.figure(figsize=(16,8))
ax = plt.axes(projection=cartopy.crs.PlateCarree(central_longitude=0.))
# overlaying the continents on top makes a nicer plot (zorder continent > zorder SSHA).
# ax.add_feature(cartopy.feature.LAND, linewidth=1, facecolor='lightgrey', edgecolor='k', zorder=1)
ax.add_feature(cartopy.feature.LAND, linewidth=1, facecolor='lightgrey', edgecolor='k')
ax.set_extent([-180, 180, lat_min, lat_max])
mymap = ax.pcolor(binnedgrid.longitude, binnedgrid.latitude, binnedgrid.binnedssha, cmap='RdYlBu_r', vmin=-0.15, vmax=0.15, shading=None, zorder=0)
# Add a colorbar to the map object to give scale
plt.colorbar(mymap, ax=ax)
# Add a title to the map object
plt.title('binned Sea surface height with respect to a mean sea surface (SSHA) [m]', size=18)
plt.show()                  








weights = np.cos(np.deg2rad(binnedgrid['latitude']))
weights.name = "weights"
ssha_weighted = binnedgrid['binnedssha'].weighted(weights)
ssha_weighted
weighted_mean = ssha_weighted.mean(("longitude", "latitude"))
print("the Global mean sea level value (with respect to a 20-year mean) for this cycle is: ", np.around(weighted_mean.values,4), " meters")








MSL_data = copernicusmarine.open_dataset(
           dataset_id = "omi_climate_sl_global_area_averaged_anomalies")

plt.scatter(MSL_data.time, MSL_data.MSL_filtered_GIA_TPA_corrected_adjusted, s=1)
plt.plot(MSL_data.time, MSL_data.trend_MSL_filtered_GIA_TPA_corrected_adjusted, 'r--')
plt.ylabel('Filtered Global Ocean MSL [cm]');



